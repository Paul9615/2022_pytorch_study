{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Train dataset and Test dataset\n",
    "+ Train dataset: 예측을 위해 사용하는 dataset \n",
    "+ Test dataset: 훈련 후 모델이 얼마나 잘 작동하는지 판별하는 dataset\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x175ed1a5b10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1234) # random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1]) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# train dataset 구성\n",
    "x_train = torch.FloatTensor([[1],[2],[3]])\n",
    "y_train = torch.FloatTensor([[2],[4],[6]])\n",
    "\n",
    "print(x_train.shape, y_train.shape) # 3 by 1 that each both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "+ linear regression의 hypothesis\n",
    "    + $y=Wx+b$ == $H(x) = Wx+b$\n",
    "    + $W$: weights (slope)\n",
    "    + $b$: bias\n",
    "\n",
    "## Cost function\n",
    "+ cost function = loss function = error function = objective function\n",
    "+ MSE(Mean Squared Error) \n",
    "    + 가설의 error의 크기를 측정하기 위함\\\n",
    "    $MSE = {1 \\over n}\\sum_{i=1}^n [y^{(i)}-H(x^{(i)})]^2$\n",
    "        + $n$: number of data\n",
    "    + regression 문제에서 적절한 $W$와 $b$를 찾기 위함\n",
    "        + 즉 MSE의 값을 최소로 만들기 위한 $W$와 $b$를 찾아 훈련에 반영하기 위함\n",
    "        + 이를 cost function으로 재정의하면 다음과 같음\\\n",
    "            $ cost(W,b) = {1 \\over n}\\sum_{i=1}^n [y^{(i)}-H(x^{(i)})]^2$\n",
    "\n",
    "## Optimizer - Gradient Descent\n",
    "+ cost function의 값을 최소로 하는 minimizer $W$와 $b$를 찾기 위한 방법으로 쓰임\n",
    "+ Gradient Descent\n",
    "    + $W$에 대한 Gradient Descent를 한다고 할 때 다음과 같다\\\n",
    "    $W \\colonequals W - \\alpha {\\frac{\\partial}{\\partial W}} cost(W)$\n",
    "        + $\\alpha$: Learning rate, $W$의 값을 변경할 때, 얼마나 크게 변경할지 결정함\n",
    "            + $\\alpha$를 지나치게 높은 값을 가질 때 찾고자 하는 $W$의 값이 발산 할 수 있음\n",
    "            + $\\alpha$를 지나치게 낮을 값을 가질 떄 학습 속도가 느려짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement linear regression with pytorch\n",
    "\n",
    "# initialize weights and bias variables\n",
    "W = torch.zeros(1, requires_grad=True) # 0으로 initialize하고 학습을 통해 변수가 변경됨\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# implement gradient for linear regression\n",
    "opt = optim.SGD([W, b], lr=0.01) # optimizer is stochastic gradient descent, learning rate set to 0.01\n",
    "\n",
    "'''\n",
    "epoch: 전체 train date가 학습에 한번 사용된 주기\n",
    "'''\n",
    "# number of epochs\n",
    "n_epoch = 1999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1999 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/1999 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/1999 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/1999 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/1999 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/1999 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/1999 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/1999 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/1999 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/1999 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/1999 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/1999 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/1999 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/1999 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/1999 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/1999 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/1999 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/1999 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/1999 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/1999 W: 1.997, b: 0.008 Cost: 0.000008\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epoch+1): \n",
    "    # calculate the hypothesis\n",
    "    hyp = x_train*W+b\n",
    "\n",
    "    # calculate cost function for linear regression\n",
    "    cost = torch.mean((hyp-y_train)**2)\n",
    "\n",
    "    '''\n",
    "    - 미분을 통해 얻은 slope를 0으로 초기화\n",
    "    : 새로운 parameter bias에 대해 새로운 slope를 구할 수 있기 때문\n",
    "    '''\n",
    "    # initialize slope to zero from calculated, avoid to cummulated values\n",
    "    opt.zero_grad()\n",
    "    # calculate slope about W and b, derivative cost function\n",
    "    cost.backward()\n",
    "    # w와 b에서 return되는 변수들의 기울기에 대해 learning rate를 곱하여 빼줌으로서 업데이트\n",
    "    opt.step()\n",
    "    # epoch 100회씩마다 관련 결과 출력 \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, n_epoch, W.item(), b.item(), cost.item())) # W가 2 그리고 b가 0에 가까운 값에 도달함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__forward 연산__\n",
    "+ $H(x)$  식에 입력 $x$로부터 예측된 $y$를 얻는 연산\n",
    "\n",
    "__backward 연산__\n",
    "+ 학습 과정에서 cost function을 미분하여 slope를 구하는 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Gradient\n",
    "+ model이 복잡해질수록 optimizer를 직접 구현해지기 어려워 짐\n",
    "+ 이에 대한 복잡함을 덜고자 auto gradient 기능을 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated:8.0\n"
     ]
    }
   ],
   "source": [
    "# declare variable randomly\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "n_epoch=100\n",
    "\n",
    "y = w**2\n",
    "z = 2*y+5\n",
    "\n",
    "# derivate about w \n",
    "z.backward()\n",
    "\n",
    "print(f'calculated:{w.grad}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariable Linear regression\n",
    "+ Hypothesis\\\n",
    "$H(x) = w_1 x_1+w_2 x_2 + ... + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 cost: 29661.800781\n",
      "Epoch 100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 cost: 1.563634\n",
      "Epoch 200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 cost: 1.497607\n",
      "Epoch 300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 cost: 1.435026\n",
      "Epoch 400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 cost: 1.375730\n",
      "Epoch 500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 cost: 1.319511\n",
      "Epoch 600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 cost: 1.266222\n",
      "Epoch 700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 cost: 1.215696\n",
      "Epoch 800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 cost: 1.167818\n",
      "Epoch 900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 cost: 1.079378\n"
     ]
    }
   ],
   "source": [
    "# train dataset\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# initialize weights and bias\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# set optimizer to SGD\n",
    "opt = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "# number of epochs\n",
    "n_epoch = 1000\n",
    "\n",
    "for epoch in range(n_epoch+1):\n",
    "    hyp = x1_train*w1 + x2_train*w2 + x3_train*w3 + b\n",
    "    cost = torch.mean((hyp-y_train)**2)\n",
    "    \n",
    "    # improving hypothesis to cost\n",
    "    opt.zero_grad()\n",
    "    cost.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if epoch % 100==0:\n",
    "        print(f'Epoch {epoch}/{n_epoch} w1: {w1.item():.3f} w2: {w2.item():.3f} w3: {w3.item():.3f} b: {b.item():.3f} cost: {cost.item():.6f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "# implement multivariable regression on vector operation\n",
    "\n",
    "# declare train data\n",
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  80], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])   # 5 by 3 \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]]) # 5 by 1 \n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare weight and bias\n",
    "w = torch.zeros((3,1), requires_grad=True) # x_train과 operation을 하려면 연산 조건(크기)에 맞아야 함\n",
    "b = torch.zeros(1,requires_grad=True) \n",
    "\n",
    "# optimizer \n",
    "opt = optim.SGD([w,b], lr=1e-5)\n",
    "\n",
    "# number of epochs\n",
    "n_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.003 cost: 29661.800781\n",
      "Epoch 1/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.005 cost: 9537.694336\n",
      "Epoch 2/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.006 cost: 3069.590088\n",
      "Epoch 3/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.007 cost: 990.670898\n",
      "Epoch 4/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.007 cost: 322.482086\n",
      "Epoch 5/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 107.717064\n",
      "Epoch 6/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 38.687496\n",
      "Epoch 7/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 16.499043\n",
      "Epoch 8/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 9.365656\n",
      "Epoch 9/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 7.071114\n",
      "Epoch 10/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 6.331847\n",
      "Epoch 11/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 6.092532\n",
      "Epoch 12/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 6.013817\n",
      "Epoch 13/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 5.986785\n",
      "Epoch 14/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 5.976325\n",
      "Epoch 15/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 5.971208\n",
      "Epoch 16/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 5.967835\n",
      "Epoch 17/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 5.964969\n",
      "Epoch 18/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 5.962291\n",
      "Epoch 19/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 5.959664\n",
      "Epoch 20/20 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.008 cost: 5.957089\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epoch+1):\n",
    "    hyp = x_train.matmul(w)+b \n",
    "    cost = torch.mean((hyp - y_train)**2)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    cost.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    print(f'Epoch {epoch}/{n_epoch} w1: {w1.item():.3f} w2: {w2.item():.3f} w3: {w3.item():.3f} b: {b.item():.3f} cost: {cost.item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the Linear regression with nn.Module\n",
    "+ pytorch에서 이미 구현되어 있는 모듈을 이용하여 linear regressio 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# set seed number for fixed random variables \n",
    "torch.manual_seed(1)\n",
    "\n",
    "# simple linear regression\n",
    "# data \n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# declare model and initialization\n",
    "mod = nn.Linear(1,1) # simple linear model이므로 input dimension과 ouput dimension은 1\n",
    "print(list(mod.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2000  cost: 13.103541\n",
      "Epoch 100/2000  cost: 1.411435\n",
      "Epoch 200/2000  cost: 0.155259\n",
      "Epoch 300/2000  cost: 0.020147\n",
      "Epoch 400/2000  cost: 0.005470\n",
      "Epoch 500/2000  cost: 0.003738\n",
      "Epoch 600/2000  cost: 0.003404\n",
      "Epoch 700/2000  cost: 0.003228\n",
      "Epoch 800/2000  cost: 0.003074\n",
      "Epoch 900/2000  cost: 0.002930\n",
      "Epoch 1000/2000  cost: 0.002792\n",
      "Epoch 1100/2000  cost: 0.002661\n",
      "Epoch 1200/2000  cost: 0.002536\n",
      "Epoch 1300/2000  cost: 0.002417\n",
      "Epoch 1400/2000  cost: 0.002304\n",
      "Epoch 1500/2000  cost: 0.002195\n",
      "Epoch 1600/2000  cost: 0.002092\n",
      "Epoch 1700/2000  cost: 0.001994\n",
      "Epoch 1800/2000  cost: 0.001901\n",
      "Epoch 1900/2000  cost: 0.001811\n",
      "Epoch 2000/2000  cost: 0.001726\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "opt = optim.SGD(mod.parameters(), lr=0.001)\n",
    "\n",
    "# number of epochs \n",
    "n_epoch = 2000\n",
    "\n",
    "for epoch in range(n_epoch+1):\n",
    "    pred = mod(x_train) # hypothesis\n",
    "    cost = F.mse_loss(pred, y_train) # MSE offer from pytorch in nn.Module\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    cost.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if epoch % 100==0:\n",
    "        print(f'Epoch {epoch}/{n_epoch}  cost: {cost.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.9167]], grad_fn=<AddmmBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[1.9518]], requires_grad=True), Parameter containing:\n",
      "tensor([0.1097], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# check variable W an b\n",
    "# declare input randomly\n",
    "n_var = torch.FloatTensor([[4.0]])\n",
    "\n",
    "# 임의 변수를 예측한 모델에 입력하여 결과 저장 \n",
    "pred_n = mod(n_var)\n",
    "\n",
    "print(pred_n)\n",
    "\n",
    "# parameters after train\n",
    "print(list(mod.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.1119,  0.2710, -0.5435]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3462], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# multivariable regression model\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "mod = nn.Linear(3,1)\n",
    "\n",
    "print(list(mod.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2000  cost: 42134.707031\n",
      "Epoch 100/2000  cost: 5.960053\n",
      "Epoch 200/2000  cost: 5.654707\n",
      "Epoch 300/2000  cost: 5.365413\n",
      "Epoch 400/2000  cost: 5.091429\n",
      "Epoch 500/2000  cost: 4.831834\n",
      "Epoch 600/2000  cost: 4.585997\n",
      "Epoch 700/2000  cost: 4.353075\n",
      "Epoch 800/2000  cost: 4.132411\n",
      "Epoch 900/2000  cost: 3.923455\n",
      "Epoch 1000/2000  cost: 3.725502\n",
      "Epoch 1100/2000  cost: 3.537972\n",
      "Epoch 1200/2000  cost: 3.360326\n",
      "Epoch 1300/2000  cost: 3.192056\n",
      "Epoch 1400/2000  cost: 3.032674\n",
      "Epoch 1500/2000  cost: 2.881700\n",
      "Epoch 1600/2000  cost: 2.738672\n",
      "Epoch 1700/2000  cost: 2.603201\n",
      "Epoch 1800/2000  cost: 2.474846\n",
      "Epoch 1900/2000  cost: 2.353286\n",
      "Epoch 2000/2000  cost: 2.238110\n"
     ]
    }
   ],
   "source": [
    "opt = optim.SGD(mod.parameters(), lr=1e-5)\n",
    "\n",
    "n_epoch = 2000\n",
    "for epoch in range(n_epoch+1):\n",
    "    pred = mod(x_train)\n",
    "    cost=F.mse_loss(pred, y_train)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    cost.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if epoch % 100==0:\n",
    "        print(f'Epoch {epoch}/{n_epoch}  cost: {cost.item():.6f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[153.7184]], grad_fn=<AddmmBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[0.8541, 0.8475, 0.3096]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3568], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# check variable W an b\n",
    "# declare input randomly\n",
    "n_var = torch.FloatTensor([[73,80,75]])\n",
    "\n",
    "# 임의 변수를 예측한 모델에 입력하여 결과 저장 \n",
    "pred_n = mod(n_var)\n",
    "\n",
    "print(pred_n)\n",
    "\n",
    "# parameters after train\n",
    "print(list(mod.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement linear regression with class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2000  cost: 13.103541\n",
      "Epoch 100/2000  cost: 13.103541\n",
      "Epoch 200/2000  cost: 13.103541\n",
      "Epoch 300/2000  cost: 13.103541\n",
      "Epoch 400/2000  cost: 13.103541\n",
      "Epoch 500/2000  cost: 13.103541\n",
      "Epoch 600/2000  cost: 13.103541\n",
      "Epoch 700/2000  cost: 13.103541\n",
      "Epoch 800/2000  cost: 13.103541\n",
      "Epoch 900/2000  cost: 13.103541\n",
      "Epoch 1000/2000  cost: 13.103541\n",
      "Epoch 1100/2000  cost: 13.103541\n",
      "Epoch 1200/2000  cost: 13.103541\n",
      "Epoch 1300/2000  cost: 13.103541\n",
      "Epoch 1400/2000  cost: 13.103541\n",
      "Epoch 1500/2000  cost: 13.103541\n",
      "Epoch 1600/2000  cost: 13.103541\n",
      "Epoch 1700/2000  cost: 13.103541\n",
      "Epoch 1800/2000  cost: 13.103541\n",
      "Epoch 1900/2000  cost: 13.103541\n",
      "Epoch 2000/2000  cost: 13.103541\n"
     ]
    }
   ],
   "source": [
    "# simple linear regression\n",
    "\n",
    "torch.manual_seed(1) # seed\n",
    "\n",
    "# declare train data\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# model class\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # nn.Module 클래스의 속성들을 가지고 initialize됨\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    \n",
    "    # model이 train data를 입력받아 forward 연산을 진행시키는 함수 \n",
    "    # model object를 data와 함께 호출하면 자동을 실행됨 \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# generate model\n",
    "mod = LinearRegression() \n",
    "# optimizer \n",
    "opti = optim.SGD(mod.parameters(), lr = 0.01)\n",
    "# number of epochs \n",
    "n_epoch = 2000\n",
    "\n",
    "for epoch in range(n_epoch+1):\n",
    "    pred = mod(x_train)\n",
    "    cost = F.mse_loss(pred, y_train)\n",
    "\n",
    "    # improve H(x) to cost\n",
    "    opt.zero_grad()\n",
    "    cost.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if epoch % 100==0:\n",
    "        print(f'Epoch {epoch}/{n_epoch}  cost: {cost.item():.6f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2000  cost: 31667.599609\n",
      "Epoch 100/2000  cost: 0.225993\n",
      "Epoch 200/2000  cost: 0.223911\n",
      "Epoch 300/2000  cost: 0.221941\n",
      "Epoch 400/2000  cost: 0.220059\n",
      "Epoch 500/2000  cost: 0.218271\n",
      "Epoch 600/2000  cost: 0.216575\n",
      "Epoch 700/2000  cost: 0.214950\n",
      "Epoch 800/2000  cost: 0.213413\n",
      "Epoch 900/2000  cost: 0.211952\n",
      "Epoch 1000/2000  cost: 0.210559\n",
      "Epoch 1100/2000  cost: 0.209230\n",
      "Epoch 1200/2000  cost: 0.207967\n",
      "Epoch 1300/2000  cost: 0.206762\n",
      "Epoch 1400/2000  cost: 0.205618\n",
      "Epoch 1500/2000  cost: 0.204529\n",
      "Epoch 1600/2000  cost: 0.203481\n",
      "Epoch 1700/2000  cost: 0.202486\n",
      "Epoch 1800/2000  cost: 0.201539\n",
      "Epoch 1900/2000  cost: 0.200634\n",
      "Epoch 2000/2000  cost: 0.199770\n"
     ]
    }
   ],
   "source": [
    "# multivariable regression model\n",
    "\n",
    "torch.manual_seed(1) # seed \n",
    "\n",
    "# data\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "\n",
    "# model class\n",
    "class MultivariableRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# generate model\n",
    "mod = MultivariableRegression()\n",
    "\n",
    "# optimizer\n",
    "opt = optim.SGD(mod.parameters(), lr = 1e-5)\n",
    "\n",
    "# number of epochs \n",
    "n_epoch = 2000\n",
    "\n",
    "for epoch in range(n_epoch+1):\n",
    "    pred = mod(x_train)\n",
    "    cost = F.mse_loss(pred, y_train)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    cost.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if epoch % 100==0:\n",
    "        print(f'Epoch {epoch}/{n_epoch}  cost: {cost.item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batch and Data Load\n",
    "+ 수십만개 이상의 전체 data에 대해 gradient descent를 수행하기엔 시간이 많이 걸림 \n",
    "+ 전체 data를 더 작은 단위로 나눠서 해당 단위로 학습하는 것이 더 효율적이므로 mini batch 개념이 등장함\n",
    "    + mini batch 학습을 하게 되면 해당 batch만큼 cost를 계산하고 gradient discent를 수행함\n",
    "    + 그리고 다음 해당 batch를 가져가서 gradient discent를 수행하고 마지막 batch까지 iteration\n",
    "    + 이러한 방법으로 전체 data에 대한 학습이 끝나면 epoch 1회가 끝남\n",
    "+ batch size는 CPU와 GPU의 memory가 2의 배수이므로 데이터 송수신의 효율을 위해 주로 2의 제곱수를 사용함 \n",
    "<br> <br>  \n",
    "+ Batch gradient discent: 전체 data에 대해 한 번에 gradient discent를 수행함\n",
    "    + 전체 data를 사용하므로 weight 값이 optimum value에 수렴하는 과정이 매우 안정적 \n",
    "    + 너무 많은 계산량\n",
    "+ Mini batch gradient discent: mini batch 단위로 gradient discent를 수행함\n",
    "    + 훈련 속도가 빠름 \n",
    "    + 단, 전체 data 중 일부만 보고 수행하여 optimum value로 수렴하는 과정에 헤멤 \n",
    "\n",
    "## Iteration \n",
    "+ if given data size: 2000 and set to batch size: 200 -> number of iterations is 10 (data size/batch size)\n",
    "    + i.e. 10 updates parameters for each epoch\n",
    "\n",
    "## Data load\n",
    "+ pytorch에서 data를 쉽게 다루기 위해 dataset과 data loader를 제공함\n",
    "    + 즉, mini batch 학습, data shuffle, parallel processing까지 간단히 처리 가능함\n",
    "    + 기본적인 사용 방법은 dataset을 define하고 이를 data loader에 전달하는 것임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 cost: 30638.056641\n",
      "Epoch    0/20 Batch 2/3 cost: 12562.412109\n",
      "Epoch    0/20 Batch 3/3 cost: 1531.081421\n",
      "Epoch    1/20 Batch 1/3 cost: 1155.684326\n",
      "Epoch    1/20 Batch 2/3 cost: 442.472046\n",
      "Epoch    1/20 Batch 3/3 cost: 56.349838\n",
      "Epoch    2/20 Batch 1/3 cost: 48.919380\n",
      "Epoch    2/20 Batch 2/3 cost: 12.958294\n",
      "Epoch    2/20 Batch 3/3 cost: 2.104483\n",
      "Epoch    3/20 Batch 1/3 cost: 3.452895\n",
      "Epoch    3/20 Batch 2/3 cost: 0.078747\n",
      "Epoch    3/20 Batch 3/3 cost: 0.084672\n",
      "Epoch    4/20 Batch 1/3 cost: 0.851869\n",
      "Epoch    4/20 Batch 2/3 cost: 0.156786\n",
      "Epoch    4/20 Batch 3/3 cost: 0.004761\n",
      "Epoch    5/20 Batch 1/3 cost: 0.577582\n",
      "Epoch    5/20 Batch 2/3 cost: 0.265107\n",
      "Epoch    5/20 Batch 3/3 cost: 0.000706\n",
      "Epoch    6/20 Batch 1/3 cost: 0.533139\n",
      "Epoch    6/20 Batch 2/3 cost: 0.289340\n",
      "Epoch    6/20 Batch 3/3 cost: 0.000344\n",
      "Epoch    7/20 Batch 1/3 cost: 0.524804\n",
      "Epoch    7/20 Batch 2/3 cost: 0.294127\n",
      "Epoch    7/20 Batch 3/3 cost: 0.000292\n",
      "Epoch    8/20 Batch 1/3 cost: 0.523086\n",
      "Epoch    8/20 Batch 2/3 cost: 0.295097\n",
      "Epoch    8/20 Batch 3/3 cost: 0.000286\n",
      "Epoch    9/20 Batch 1/3 cost: 0.522628\n",
      "Epoch    9/20 Batch 2/3 cost: 0.295328\n",
      "Epoch    9/20 Batch 3/3 cost: 0.000288\n",
      "Epoch   10/20 Batch 1/3 cost: 0.522401\n",
      "Epoch   10/20 Batch 2/3 cost: 0.295406\n",
      "Epoch   10/20 Batch 3/3 cost: 0.000292\n",
      "Epoch   11/20 Batch 1/3 cost: 0.522197\n",
      "Epoch   11/20 Batch 2/3 cost: 0.295458\n",
      "Epoch   11/20 Batch 3/3 cost: 0.000295\n",
      "Epoch   12/20 Batch 1/3 cost: 0.522034\n",
      "Epoch   12/20 Batch 2/3 cost: 0.295503\n",
      "Epoch   12/20 Batch 3/3 cost: 0.000299\n",
      "Epoch   13/20 Batch 1/3 cost: 0.521868\n",
      "Epoch   13/20 Batch 2/3 cost: 0.295574\n",
      "Epoch   13/20 Batch 3/3 cost: 0.000304\n",
      "Epoch   14/20 Batch 1/3 cost: 0.521720\n",
      "Epoch   14/20 Batch 2/3 cost: 0.295609\n",
      "Epoch   14/20 Batch 3/3 cost: 0.000308\n",
      "Epoch   15/20 Batch 1/3 cost: 0.521572\n",
      "Epoch   15/20 Batch 2/3 cost: 0.295663\n",
      "Epoch   15/20 Batch 3/3 cost: 0.000312\n",
      "Epoch   16/20 Batch 1/3 cost: 0.521391\n",
      "Epoch   16/20 Batch 2/3 cost: 0.295722\n",
      "Epoch   16/20 Batch 3/3 cost: 0.000316\n",
      "Epoch   17/20 Batch 1/3 cost: 0.521224\n",
      "Epoch   17/20 Batch 2/3 cost: 0.295767\n",
      "Epoch   17/20 Batch 3/3 cost: 0.000320\n",
      "Epoch   18/20 Batch 1/3 cost: 0.521058\n",
      "Epoch   18/20 Batch 2/3 cost: 0.295819\n",
      "Epoch   18/20 Batch 3/3 cost: 0.000324\n",
      "Epoch   19/20 Batch 1/3 cost: 0.520858\n",
      "Epoch   19/20 Batch 2/3 cost: 0.295857\n",
      "Epoch   19/20 Batch 3/3 cost: 0.000327\n",
      "Epoch   20/20 Batch 1/3 cost: 0.520714\n",
      "Epoch   20/20 Batch 2/3 cost: 0.295915\n",
      "Epoch   20/20 Batch 3/3 cost: 0.000332\n"
     ]
    }
   ],
   "source": [
    "from doctest import script_from_examples\n",
    "from torch.utils.data import TensorDataset # tesor dataset\n",
    "from torch.utils.data import DataLoader  # data loader\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# data \n",
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "\n",
    "# print(x_train.size())\n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "# print(y_train.size())\n",
    "\n",
    "# use input to TensorDataset and save to dataset\n",
    "ds = TensorDataset(x_train, y_train)\n",
    "\n",
    "# data loader\n",
    "'''\n",
    "- data loader는 기본적으로 2개의 인자를 받음\n",
    "    - dataset, batch size\n",
    "    - batch size는 통상적으로 2의 배수 사용\n",
    "    - shuffle을 true로 하면 각 epoch마다 dataset을 섞어 data가 학습되는 순서를 바꿈\n",
    "'''\n",
    "# dl의 길이(number of iterations)가 3인 이유\n",
    "dl = DataLoader(ds, batch_size=2, shuffle=False)\n",
    "# print(list(dl))\n",
    "\n",
    "# model \n",
    "mod = nn.Linear(3,1)\n",
    "# optimizer\n",
    "opt = optim.SGD(mod.parameters(), lr=1e-5)\n",
    "# number of epochs\n",
    "n_ep = 20\n",
    "\n",
    "for epoch in range(n_ep+1):\n",
    "    for batch_idx, samples in enumerate(dl):\n",
    "        x_train, y_train = samples\n",
    "        pred = mod(x_train)\n",
    "        cost = F.mse_loss(pred, y_train)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        cost.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        print(f'Epoch {epoch:4d}/{n_ep} Batch {batch_idx+1}/{len(dl)} cost: {cost.item():.6f}')    # batch에서 왜 3이 나왔을까?\n",
    "        \n",
    "        '''\n",
    "        - dl의 길이(number of iterations)가 3인 이유\n",
    "            - batch size는 2이고 x train은 (5,3), y train은 (5,1)이므로 데이터의 크기는 5 \n",
    "            - data size / batch size = 2.5이지만 2.5번 반복으론 전체 data를 순회할 수 없으므로 3번 반복함\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset\n",
    "+ torch.utils.data.Dataset을 상속받아 직접 custom dataset을 만드는 경우가 있음 \n",
    "    + torch.utils.data.Dataset은 pytorch에서 dataset을 제공하는 abstractive class\n",
    "+ dataset을 상속받아 다음 method들을 override 하여 custom dataset을 만들 수 있다.\n",
    "\n",
    "__key-terminal__\n",
    "+ override: 상위 class에서 정의된 variable과 method의 내용을 하위 클래스에서 변경하여 재정의하는 것\n",
    "    + method의 이름, parameter의 갯수나 type이 동일해야 함\n",
    "    + 주로 상위 class의 동작을 상속받은 하위 class에서 변경하기 위해 사용됨\n",
    "    + 상속 관계에서 만 가능  \n",
    "+ overloading: 하나의 class 내 또는 상속 관계에 있는 class 간에 같은 이름을 갖는 method가 다른 작업을 할 수 있는 것\n",
    "    + method의 이름은 같고 parameter의 갯수나 type이 다른 함수를 정의하는 것\n",
    "    + 동일 클래스 내 또는 상속 관계 둘 다 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# simple linear regression with custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    # part of preprocessing dataset\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                   [93, 88, 93],\n",
    "                   [89, 91, 90],\n",
    "                   [96, 98, 100],\n",
    "                   [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "        \n",
    "    # return the number of overall data\n",
    "    # 총 sample의 수를 적어주는 부분\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "        \n",
    "    # index를 입력받아 mapping되는 입출력의 data를 tensor 형태로 return\n",
    "    # dataset에서 특정 1개의 sample을 가져오는 함수\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 cost: 44327.171875\n",
      "Epoch    0/20 Batch 2/3 cost: 10033.855469\n",
      "Epoch    0/20 Batch 3/3 cost: 2258.893311\n",
      "Epoch    1/20 Batch 1/3 cost: 1216.072144\n",
      "Epoch    1/20 Batch 2/3 cost: 557.448486\n",
      "Epoch    1/20 Batch 3/3 cost: 262.458679\n",
      "Epoch    2/20 Batch 1/3 cost: 39.818665\n",
      "Epoch    2/20 Batch 2/3 cost: 15.440739\n",
      "Epoch    2/20 Batch 3/3 cost: 2.757791\n",
      "Epoch    3/20 Batch 1/3 cost: 12.319376\n",
      "Epoch    3/20 Batch 2/3 cost: 2.018374\n",
      "Epoch    3/20 Batch 3/3 cost: 15.770846\n",
      "Epoch    4/20 Batch 1/3 cost: 4.162767\n",
      "Epoch    4/20 Batch 2/3 cost: 6.820552\n",
      "Epoch    4/20 Batch 3/3 cost: 16.022591\n",
      "Epoch    5/20 Batch 1/3 cost: 7.107206\n",
      "Epoch    5/20 Batch 2/3 cost: 4.565801\n",
      "Epoch    5/20 Batch 3/3 cost: 15.153234\n",
      "Epoch    6/20 Batch 1/3 cost: 4.444887\n",
      "Epoch    6/20 Batch 2/3 cost: 6.947250\n",
      "Epoch    6/20 Batch 3/3 cost: 10.440168\n",
      "Epoch    7/20 Batch 1/3 cost: 2.658041\n",
      "Epoch    7/20 Batch 2/3 cost: 17.536879\n",
      "Epoch    7/20 Batch 3/3 cost: 4.148447\n",
      "Epoch    8/20 Batch 1/3 cost: 8.140711\n",
      "Epoch    8/20 Batch 2/3 cost: 4.479380\n",
      "Epoch    8/20 Batch 3/3 cost: 12.304320\n",
      "Epoch    9/20 Batch 1/3 cost: 6.948590\n",
      "Epoch    9/20 Batch 2/3 cost: 6.939762\n",
      "Epoch    9/20 Batch 3/3 cost: 4.587046\n",
      "Epoch   10/20 Batch 1/3 cost: 8.149935\n",
      "Epoch   10/20 Batch 2/3 cost: 4.360662\n",
      "Epoch   10/20 Batch 3/3 cost: 7.902605\n",
      "Epoch   11/20 Batch 1/3 cost: 1.789071\n",
      "Epoch   11/20 Batch 2/3 cost: 18.581242\n",
      "Epoch   11/20 Batch 3/3 cost: 3.876457\n",
      "Epoch   12/20 Batch 1/3 cost: 5.154574\n",
      "Epoch   12/20 Batch 2/3 cost: 4.589956\n",
      "Epoch   12/20 Batch 3/3 cost: 17.691410\n",
      "Epoch   13/20 Batch 1/3 cost: 7.450467\n",
      "Epoch   13/20 Batch 2/3 cost: 5.332194\n",
      "Epoch   13/20 Batch 3/3 cost: 12.392900\n",
      "Epoch   14/20 Batch 1/3 cost: 10.964632\n",
      "Epoch   14/20 Batch 2/3 cost: 7.588781\n",
      "Epoch   14/20 Batch 3/3 cost: 5.734437\n",
      "Epoch   15/20 Batch 1/3 cost: 8.326908\n",
      "Epoch   15/20 Batch 2/3 cost: 9.566439\n",
      "Epoch   15/20 Batch 3/3 cost: 6.309114\n",
      "Epoch   16/20 Batch 1/3 cost: 10.774476\n",
      "Epoch   16/20 Batch 2/3 cost: 10.607923\n",
      "Epoch   16/20 Batch 3/3 cost: 7.151723\n",
      "Epoch   17/20 Batch 1/3 cost: 6.678420\n",
      "Epoch   17/20 Batch 2/3 cost: 2.244556\n",
      "Epoch   17/20 Batch 3/3 cost: 16.589190\n",
      "Epoch   18/20 Batch 1/3 cost: 9.432505\n",
      "Epoch   18/20 Batch 2/3 cost: 4.104985\n",
      "Epoch   18/20 Batch 3/3 cost: 4.294538\n",
      "Epoch   19/20 Batch 1/3 cost: 5.095177\n",
      "Epoch   19/20 Batch 2/3 cost: 7.104434\n",
      "Epoch   19/20 Batch 3/3 cost: 7.975780\n",
      "Epoch   20/20 Batch 1/3 cost: 1.911605\n",
      "Epoch   20/20 Batch 2/3 cost: 10.471613\n",
      "Epoch   20/20 Batch 3/3 cost: 9.444724\n"
     ]
    }
   ],
   "source": [
    "ds = CustomDataset()\n",
    "dl = DataLoader(ds, batch_size=2, shuffle=True)\n",
    "\n",
    "mod = nn.Linear(3,1)\n",
    "opt = optim.SGD(mod.parameters(), lr=1e-5)\n",
    "\n",
    "n_epoch = 20 \n",
    "\n",
    "for epoch in range(n_epoch+1):\n",
    "    for batch_idx, samples in enumerate(dl):\n",
    "        x_train, y_train = samples\n",
    "        pred = mod(x_train)\n",
    "        cost = F.mse_loss(pred, y_train)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        cost.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        print(f'Epoch {epoch:4d}/{n_ep} Batch {batch_idx+1}/{len(dl)} cost: {cost.item():.6f}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1e5884059a9e58218085a732887c585e94818ab84f9336d4151624cf315f1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
