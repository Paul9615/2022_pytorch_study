{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나머지는 참고자료로 읽어보기!!! (시간날 때 정리)\n",
    "+ 비선형~ \n",
    "+ MLP로 손글씨, MNIST 분류는 Colab으로 실습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "__Overfitting__\n",
    "+ model의 training이 과하게 학습된 경우로 test data나 실제 service에 대해 정확도가 좋지 않은 현상이 발생할 수 있음\n",
    "    + 즉, training data에 대해 error가 낮아지지만, test data에 대해 error가 높아짐\n",
    "+ test data의 error가 증가하기 전이나, accuracy가 감소하기 전에 training을 멈추는 것이 바람직함\n",
    "+ 방지를 위해 test data의 성능이 낮아지기 전에 training을 멈추는 것이 좋지만 underfitting 위험이 있음\n",
    "    + underfitting보다 overfitting보다 accuracy가 낮음\n",
    "    + 방지 방법으로 drop out과 early stopping이 있음\n",
    "\n",
    "## Perceptron\n",
    "+ 다수의 입력으로부터 하나의 결과를 내보내는 algorithm\n",
    "+ input value를 가중치와 함께 neural에 전달함\n",
    "    + 각 input value에는 각각의 가중치가 존재함\n",
    "        + 각 input value이 가중치와 곱해져 nueral에 보내짐\n",
    "        + 곱의 전체 합이 threshold를 넘으면 출력 신호 1 또는 그렇지 않은 경우 0을 출력함(step function)  \n",
    "        + 가중치가 클수록 해당 input value가 중요해짐 \n",
    "\n",
    "### Single layer perceptron\n",
    "+ input layer와 output layer로만 이루어져 있음\n",
    "+ single layer perceptron을 이용해서 AND, NAND, 그리고 OR gate를 쉽게 구현할 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AND gate\n",
    "# 둘 다 1이면 1\n",
    "def AND_gate(x1,x2):\n",
    "    w1 = 0.5\n",
    "    w2 = 0.5\n",
    "    b = - 0.7\n",
    "    res = x1*w1 + x2*w2 + b\n",
    "    if res <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "AND_gate(0, 0), AND_gate(0, 1), AND_gate(1, 0), AND_gate(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 1, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OR gate \n",
    "# 둘 중 하나가 1이 또는 둘 다 1이면 1 \n",
    "def OR_gate(x1, x2):\n",
    "    w1 = 0.6\n",
    "    w2 = 0.6\n",
    "    b = - 0.5\n",
    "    res = x1*w1 + x2*w2 + b\n",
    "    if res <= 0:\n",
    "        return 0\n",
    "    else: \n",
    "        return 1 \n",
    "OR_gate(0, 0), OR_gate(0, 1), OR_gate(1, 0), OR_gate(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NAND gate\n",
    "# 둘다 1일 경우 0\n",
    "def NAND_gate(x1,x2):\n",
    "    w1 = -0.5\n",
    "    w2 = -0.5 \n",
    "    b = 0.7 \n",
    "    res = x1*w1 + x2*w2 + b\n",
    "    if res <= 0:\n",
    "        return 0 \n",
    "    else:\n",
    "        return 1 \n",
    "\n",
    "NAND_gate(0, 0), NAND_gate(0, 1), NAND_gate(1, 0), NAND_gate(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR gate\n",
    "+ 입력 두 개가 서로 다른 값을 갖고 있을 때만 출력값이 1\n",
    "+ AND, NAND, OR gate와 달리 single layer perceptron으로 구현하기 힘듦 \n",
    "    + single layer perceptron은 직선 하나로 두 영역을 나눌 수 있는 문제에 대해서 구현 가능\n",
    "+ AND, NAND, OR gate를 조합하면 만들 수 있음\n",
    "    + layer를 더 쌓으면 구현 가능함 -> multi-layer perceptron \n",
    "\n",
    "## Multi-layer perceptron(MLP)\n",
    "+ single-layer perceptron와 달리 입력층과 출력층 사이에 층을 추가함(Hidden layer)\n",
    "    + MLP는 hidden layer가 1개 이상인 perceptron임\n",
    "    + hidden layer가 2개 이상인 neural network를 Deep Neural Network(DNN)라고 함\n",
    "        + training시키는 ANN이 DNN일 경우 Deep learning임\n",
    "\n",
    "__Training__\n",
    "+ machine이 weight를 스스로 찾아내도록 자동화시키야 하는 단계\n",
    "\n",
    "## XOR problem with Single layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code just setting for run on GPU, now variable 'device' state is CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR data \n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# set to single layer perceptron, while activation function is sigmoid function\n",
    "linear = nn.Linear(2,1, bias = True)\n",
    "sig = nn.Sigmoid()\n",
    "mod = nn.Sequential(linear, sig).to(device)\n",
    "\n",
    "# cost function(cross entropy(binary classification problem)) and optimizer\n",
    "criterion = nn.BCELoss().to(device)\n",
    "opt = torch.optim.SGD(mod.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7018222212791443\n",
      "100 0.6931473016738892\n",
      "200 0.6931471824645996\n",
      "300 0.6931471824645996\n",
      "400 0.6931471824645996\n",
      "500 0.6931471824645996\n",
      "600 0.6931471824645996\n",
      "700 0.6931471824645996\n",
      "800 0.6931471824645996\n",
      "900 0.6931471824645996\n",
      "1000 0.6931471824645996\n",
      "1100 0.6931471824645996\n",
      "1200 0.6931471824645996\n",
      "1300 0.6931471824645996\n",
      "1400 0.6931471824645996\n",
      "1500 0.6931471824645996\n",
      "1600 0.6931471824645996\n",
      "1700 0.6931471824645996\n",
      "1800 0.6931471824645996\n",
      "1900 0.6931471824645996\n",
      "2000 0.6931471824645996\n",
      "2100 0.6931471824645996\n",
      "2200 0.6931471824645996\n",
      "2300 0.6931471824645996\n",
      "2400 0.6931471824645996\n",
      "2500 0.6931471824645996\n",
      "2600 0.6931471824645996\n",
      "2700 0.6931471824645996\n",
      "2800 0.6931471824645996\n",
      "2900 0.6931471824645996\n",
      "3000 0.6931471824645996\n",
      "3100 0.6931471824645996\n",
      "3200 0.6931471824645996\n",
      "3300 0.6931471824645996\n",
      "3400 0.6931471824645996\n",
      "3500 0.6931471824645996\n",
      "3600 0.6931471824645996\n",
      "3700 0.6931471824645996\n",
      "3800 0.6931471824645996\n",
      "3900 0.6931471824645996\n",
      "4000 0.6931471824645996\n",
      "4100 0.6931471824645996\n",
      "4200 0.6931471824645996\n",
      "4300 0.6931471824645996\n",
      "4400 0.6931471824645996\n",
      "4500 0.6931471824645996\n",
      "4600 0.6931471824645996\n",
      "4700 0.6931471824645996\n",
      "4800 0.6931471824645996\n",
      "4900 0.6931471824645996\n",
      "5000 0.6931471824645996\n",
      "5100 0.6931471824645996\n",
      "5200 0.6931471824645996\n",
      "5300 0.6931471824645996\n",
      "5400 0.6931471824645996\n",
      "5500 0.6931471824645996\n",
      "5600 0.6931471824645996\n",
      "5700 0.6931471824645996\n",
      "5800 0.6931471824645996\n",
      "5900 0.6931471824645996\n",
      "6000 0.6931471824645996\n",
      "6100 0.6931471824645996\n",
      "6200 0.6931471824645996\n",
      "6300 0.6931471824645996\n",
      "6400 0.6931471824645996\n",
      "6500 0.6931471824645996\n",
      "6600 0.6931471824645996\n",
      "6700 0.6931471824645996\n",
      "6800 0.6931471824645996\n",
      "6900 0.6931471824645996\n",
      "7000 0.6931471824645996\n",
      "7100 0.6931471824645996\n",
      "7200 0.6931471824645996\n",
      "7300 0.6931471824645996\n",
      "7400 0.6931471824645996\n",
      "7500 0.6931471824645996\n",
      "7600 0.6931471824645996\n",
      "7700 0.6931471824645996\n",
      "7800 0.6931471824645996\n",
      "7900 0.6931471824645996\n",
      "8000 0.6931471824645996\n",
      "8100 0.6931471824645996\n",
      "8200 0.6931471824645996\n",
      "8300 0.6931471824645996\n",
      "8400 0.6931471824645996\n",
      "8500 0.6931471824645996\n",
      "8600 0.6931471824645996\n",
      "8700 0.6931471824645996\n",
      "8800 0.6931471824645996\n",
      "8900 0.6931471824645996\n",
      "9000 0.6931471824645996\n",
      "9100 0.6931471824645996\n",
      "9200 0.6931471824645996\n",
      "9300 0.6931471824645996\n",
      "9400 0.6931471824645996\n",
      "9500 0.6931471824645996\n",
      "9600 0.6931471824645996\n",
      "9700 0.6931471824645996\n",
      "9800 0.6931471824645996\n",
      "9900 0.6931471824645996\n",
      "10000 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "# run about epoch 10000 times\n",
    "for step in range(10001):\n",
    "    opt.zero_grad()\n",
    "    hyp = mod(X)\n",
    "    \n",
    "    # cost function \n",
    "    cost = criterion(hyp, Y)\n",
    "    cost.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())\n",
    "        \n",
    "# 10000번 epoch가 되어도 cost가 줄어들지 않는 이유는 single layer perceptron으로 XOR 문제를 풀 수 없기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Predicted: \n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Real values(Y): \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy: \n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "# check predicted values\n",
    "with torch.no_grad():\n",
    "    hyp = mod(X)\n",
    "    pred = (hyp>0.5).float()\n",
    "    acc = (pred==Y).float().mean()\n",
    "    print(f'Hypothesis: \\n {hyp.detach().cpu().numpy()}')\n",
    "    print(f'Predicted: \\n {pred.detach().cpu().numpy()}')\n",
    "    print(f'Real values(Y): \\n {Y.cpu().numpy()}')\n",
    "    print(f'Accuracy: \\n {acc.item()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackPropagation\n",
    "+ forward propagation을 통해 산출된 output을 실제값과의 error가 최소가 되도록 \n",
    "+ 각 layer에서 입력되는 값에 곱해지는 weighted와 bias를 계산하여 업데이트함\\\n",
    "참고: https://m.blog.naver.com/samsjang/221033626685\n",
    "### Forward Propagation\n",
    "<img src=\"https://wikidocs.net/images/page/37406/nn2_final_final.PNG\">\n",
    "\n",
    "+ 위 그림에서 $x_1, x_2$는 입력값이고 $o_1, o_2$은 출력값일 때, \n",
    "    + 각 입력은 input layer에서 hidden layer로 향하면서 각 가중치 $w_i$를 곱해지고 \n",
    "    + 가중합으로 계산되어 hidden layer의 sigmoid function의 입력값이 됨($z_1, z_2$)\n",
    "        + e.g. $z_1 = w_1x_1+w_2x_2, \\ z_2 = w_3x_3+w_4x_4 $\n",
    "    + sigmoid fuction에서 return된 값 $h_1, \\ h_2$는 다음과 같다\n",
    "        + e.g. $h_1 = sigmoid(z_1), \\ h_2 = sigmoid(z_2)$\n",
    "    + return된 값을 output layer로 향하게 되고 다시 각 해당 가중치와 곱해져 가중합이 되며, sigmoid function의 입력값이 됨\n",
    "        + e.g. $z_3 = w_5h_1+w_6h_2, \\ z_4 = w_7h_1+w_8h_2$\n",
    "    + 계산된 $z_3, \\ z_4$는 output layer에서 sigmoid function을 통해 최종 출력값 $o_1, \\ o_2$가 됨\n",
    "        + e.g. $o_1 = sigmoid(z_3), \\ o_2 = sigmoid(z_4)$ \n",
    "+ 최종값인 출력값 $o_1, \\ o_2$(예측값)는 error를 계산하기 위한 loss function으로 MSE를 이용함\n",
    "    + e.g. $E_{o_1} = \\frac{1}{2} (target_{o_1}-output_{o_1})^2, \\ E_{o_2} = \\frac{1}{2} (target_{o_2}-output_{o_2})^2, \\ E_{total}=\\sum_{i=1}^N E_{o_i}$\n",
    "\n",
    "### Back Propagation\n",
    "+ back propagation은 output layer에서 input layer로 계산하면서 weighted를 update함(forward -> back)\n",
    "    + output layer 이전의 hidden layer를 N층이라고 하면 output layer와 N층 사이의 weighted를 업데이트(back propagation 1 step)\n",
    "    + 그 다음 N층과 N층 사이의 weighted를 update하면서 나머지 weighted를 update해 나감(back propagation 2 step)\n",
    "\n",
    "__Back propagation 1 step__\\\n",
    "<img src=\"https://wikidocs.net/images/page/37406/nn3_final.PNG\">\n",
    "\n",
    "+ chain rule을 이용함 \n",
    "\n",
    "+ 위 그림에서 가중치 $w_5, w_6, w_7, w_8$을 update해야함. \n",
    "    + e.g. 가중치 $w_5$에 대한 update \n",
    "        + $\\frac{\\partial E_{total}}{\\partial w_5} = \\frac{\\partial E_{total}}{\\partial o_1} \\times \\frac{\\partial o_1}{\\partial z_3} \\times \\frac{\\partial z_3}{\\partial w_5}$\n",
    "        + 다음으로 update하기 위한 gradient descent(optimization algorithm 이용)\n",
    "            + $w_5^+ = w_5 - \\alpha \\frac{\\partial E_{total}}{{\\partial w_5}}$\n",
    "+ 나머지도 같은 방식으로 weighted를 update한다\n",
    "\n",
    "__Back propagation 2 step__\\\n",
    "<img src=\"https://wikidocs.net/images/page/37406/nn4.PNG\">\n",
    "\n",
    "+ e.g. 가중치 $w_1$에 대한 update\n",
    "    + $\\frac{\\partial E_{total}}{\\partial w_1} = \\frac{\\partial e_{total}}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial z_1} \\times \\frac{\\partial z_1}{\\partial w_1}$\n",
    "    + 그 다음으로 update를 위해 gradient descent를 함\n",
    "        + $w_5^+ = w_5 - \\alpha \\frac{\\partial E_{total}}{\\partial w_5}$\n",
    "    + 이외 다른 weighted도 같은 방식으로 계산\n",
    "\n",
    "\n",
    "__back propagation을 진행 후 error가 감소했는지 확인하기 위해 forward propagation을 진행하여 산출한 predicted와 real value의 error를 확인__\n",
    "\n",
    "## solve XOR Problem with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR data\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 0.7555788159370422 \n",
      "Epoch: 100 Cost: 0.6931402683258057 \n",
      "Epoch: 200 Cost: 0.6931398510932922 \n",
      "Epoch: 300 Cost: 0.6931394338607788 \n",
      "Epoch: 400 Cost: 0.6931390762329102 \n",
      "Epoch: 500 Cost: 0.6931387186050415 \n",
      "Epoch: 600 Cost: 0.6931383013725281 \n",
      "Epoch: 700 Cost: 0.6931377649307251 \n",
      "Epoch: 800 Cost: 0.6931372880935669 \n",
      "Epoch: 900 Cost: 0.6931368708610535 \n",
      "Epoch: 1000 Cost: 0.6931363344192505 \n",
      "Epoch: 1100 Cost: 0.6931357979774475 \n",
      "Epoch: 1200 Cost: 0.6931352615356445 \n",
      "Epoch: 1300 Cost: 0.6931347250938416 \n",
      "Epoch: 1400 Cost: 0.693134069442749 \n",
      "Epoch: 1500 Cost: 0.6931334733963013 \n",
      "Epoch: 1600 Cost: 0.693132758140564 \n",
      "Epoch: 1700 Cost: 0.6931321620941162 \n",
      "Epoch: 1800 Cost: 0.6931313276290894 \n",
      "Epoch: 1900 Cost: 0.6931305527687073 \n",
      "Epoch: 2000 Cost: 0.6931297779083252 \n",
      "Epoch: 2100 Cost: 0.6931288838386536 \n",
      "Epoch: 2200 Cost: 0.6931278705596924 \n",
      "Epoch: 2300 Cost: 0.693126916885376 \n",
      "Epoch: 2400 Cost: 0.69312584400177 \n",
      "Epoch: 2500 Cost: 0.6931246519088745 \n",
      "Epoch: 2600 Cost: 0.6931235194206238 \n",
      "Epoch: 2700 Cost: 0.693122148513794 \n",
      "Epoch: 2800 Cost: 0.6931207180023193 \n",
      "Epoch: 2900 Cost: 0.6931192278862 \n",
      "Epoch: 3000 Cost: 0.6931175589561462 \n",
      "Epoch: 3100 Cost: 0.6931157112121582 \n",
      "Epoch: 3200 Cost: 0.6931137442588806 \n",
      "Epoch: 3300 Cost: 0.6931116580963135 \n",
      "Epoch: 3400 Cost: 0.693109393119812 \n",
      "Epoch: 3500 Cost: 0.6931067705154419 \n",
      "Epoch: 3600 Cost: 0.6931039690971375 \n",
      "Epoch: 3700 Cost: 0.6931008100509644 \n",
      "Epoch: 3800 Cost: 0.6930973529815674 \n",
      "Epoch: 3900 Cost: 0.6930933594703674 \n",
      "Epoch: 4000 Cost: 0.6930890679359436 \n",
      "Epoch: 4100 Cost: 0.6930840015411377 \n",
      "Epoch: 4200 Cost: 0.6930784583091736 \n",
      "Epoch: 4300 Cost: 0.6930720806121826 \n",
      "Epoch: 4400 Cost: 0.6930646300315857 \n",
      "Epoch: 4500 Cost: 0.693056046962738 \n",
      "Epoch: 4600 Cost: 0.693045973777771 \n",
      "Epoch: 4700 Cost: 0.6930340528488159 \n",
      "Epoch: 4800 Cost: 0.6930198073387146 \n",
      "Epoch: 4900 Cost: 0.693002462387085 \n",
      "Epoch: 5000 Cost: 0.6929813027381897 \n",
      "Epoch: 5100 Cost: 0.6929546594619751 \n",
      "Epoch: 5200 Cost: 0.6929207444190979 \n",
      "Epoch: 5300 Cost: 0.6928763389587402 \n",
      "Epoch: 5400 Cost: 0.6928168535232544 \n",
      "Epoch: 5500 Cost: 0.692733883857727 \n",
      "Epoch: 5600 Cost: 0.6926131248474121 \n",
      "Epoch: 5700 Cost: 0.692427396774292 \n",
      "Epoch: 5800 Cost: 0.6921188831329346 \n",
      "Epoch: 5900 Cost: 0.6915476322174072 \n",
      "Epoch: 6000 Cost: 0.6903036236763 \n",
      "Epoch: 6100 Cost: 0.6867527961730957 \n",
      "Epoch: 6200 Cost: 0.6699885129928589 \n",
      "Epoch: 6300 Cost: 0.5370907783508301 \n",
      "Epoch: 6400 Cost: 0.044061824679374695 \n",
      "Epoch: 6500 Cost: 0.00923094805330038 \n",
      "Epoch: 6600 Cost: 0.004695628769695759 \n",
      "Epoch: 6700 Cost: 0.0030650317203253508 \n",
      "Epoch: 6800 Cost: 0.002247381955385208 \n",
      "Epoch: 6900 Cost: 0.0017620802391320467 \n",
      "Epoch: 7000 Cost: 0.0014430213486775756 \n",
      "Epoch: 7100 Cost: 0.0012182630598545074 \n",
      "Epoch: 7200 Cost: 0.0010518796043470502 \n",
      "Epoch: 7300 Cost: 0.0009240640792995691 \n",
      "Epoch: 7400 Cost: 0.0008230196544900537 \n",
      "Epoch: 7500 Cost: 0.0007411178667098284 \n",
      "Epoch: 7600 Cost: 0.0006735525093972683 \n",
      "Epoch: 7700 Cost: 0.0006169209955260158 \n",
      "Epoch: 7800 Cost: 0.0005687465309165418 \n",
      "Epoch: 7900 Cost: 0.0005273133283481002 \n",
      "Epoch: 8000 Cost: 0.0004913532757200301 \n",
      "Epoch: 8100 Cost: 0.00045980728464201093 \n",
      "Epoch: 8200 Cost: 0.0004319594008848071 \n",
      "Epoch: 8300 Cost: 0.0004071533912792802 \n",
      "Epoch: 8400 Cost: 0.000385016348445788 \n",
      "Epoch: 8500 Cost: 0.0003650262951850891 \n",
      "Epoch: 8600 Cost: 0.0003469894581940025 \n",
      "Epoch: 8700 Cost: 0.00033059256384149194 \n",
      "Epoch: 8800 Cost: 0.0003156120073981583 \n",
      "Epoch: 8900 Cost: 0.00030188378877937794 \n",
      "Epoch: 9000 Cost: 0.00028928855317644775 \n",
      "Epoch: 9100 Cost: 0.0002776623296085745 \n",
      "Epoch: 9200 Cost: 0.00026693055406212807 \n",
      "Epoch: 9300 Cost: 0.0002569292555563152 \n",
      "Epoch: 9400 Cost: 0.00024764344561845064 \n",
      "Epoch: 9500 Cost: 0.0002389986620983109 \n",
      "Epoch: 9600 Cost: 0.00023095012875273824 \n",
      "Epoch: 9700 Cost: 0.00022334880486596376 \n",
      "Epoch: 9800 Cost: 0.00021622446365654469 \n",
      "Epoch: 9900 Cost: 0.0002095622185152024 \n",
      "Epoch: 10000 Cost: 0.0002032577176578343 \n"
     ]
    }
   ],
   "source": [
    "# model(MLP)\n",
    "mod = nn.Sequential(\n",
    "    nn.Linear(2,10,bias = True), # input layer = 2, hidden layer = 10 \n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10,10,bias = True), # hidden layer = 10, hidden layer = 10\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10,10,bias = True), # hidden layer = 10, hidden layer = 10\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10,1,bias = True), # input layer = 10, outpt layer = 1\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# define cost function, optimier and number of epochs\n",
    "criterion = nn.BCELoss()\n",
    "opt = torch.optim.SGD(mod.parameters(), lr=1)\n",
    "n_epoch = 10000\n",
    "\n",
    "# iterate as much as epoch with back propagation\n",
    "\n",
    "for epoch in range(n_epoch+1):\n",
    "    opt.zero_grad()\n",
    "    # forward operation\n",
    "    hyp = mod(X)\n",
    "    \n",
    "    cost = criterion(hyp,Y)\n",
    "    cost.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch} Cost: {cost.item()} ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs from model(hypothesis): \n",
      " [[1.9867101e-04]\n",
      " [9.9977762e-01]\n",
      " [9.9982017e-01]\n",
      " [2.1193606e-04]]\n",
      "Predicted value from model: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Real values: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Accuracy: \n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate predicted value from trained MLP\n",
    "with torch.no_grad():\n",
    "    hyp = mod(X)\n",
    "    pred = (hyp > 0.5).float()\n",
    "    acc = (pred == Y).float().mean()\n",
    "    print(f'Outputs from model(hypothesis): \\n {hyp.detach().cpu().numpy()}')\n",
    "    print(f'Predicted value from model: \\n {pred.detach().cpu().numpy()}')\n",
    "    print(f'Real values: \\n {Y.cpu().numpy()}')\n",
    "    print(f'Accuracy: \\n {acc.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1e5884059a9e58218085a732887c585e94818ab84f9336d4151624cf315f1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
